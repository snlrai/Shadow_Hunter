"""
Shadow Hunter â€” Unified Local Entry Point
Bootstraps the entire event-driven architecture for local development.

This is the "one command to run everything" entry point that wires up:
  - MemoryBroker (Pub/Sub)
  - SQLiteGraphStore (Persistence)
  - AnalyzerEngine (Hybrid Brain)
  - ActiveProbe (Active Defense)
  - ResponseManager (Auto-Response)
  - Data Ingestion (Simulation or Live)

Usage:
    python run_local.py --sim          # Simulate traffic from CSV
    python run_local.py --sim --reset  # Reset DB and re-simulate
    python run_local.py --live         # Live packet capture (needs Npcap)
"""

import argparse
import logging
import sys
import time
from datetime import datetime

# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(name)-32s â”‚ %(levelname)-7s â”‚ %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("shadow_hunter.main")

# â”€â”€ Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pkg.infra.local.broker import MemoryBroker
from pkg.infra.local.sqlite_store import SQLiteGraphStore

# â”€â”€ Services â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from services.analyzer.engine import AnalyzerEngine
from services.active_defense.interrogator import ActiveProbe
from services.response.manager import ResponseManager

# â”€â”€ Data Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from traffic_simulator import generate_dataset


def banner():
    """Print startup banner."""
    print("""
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚           SHADOW HUNTER â€” Unified Architecture         â”‚
 â”‚          Event-Driven â€¢ ML â€¢ Active Defense              â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚  Heuristics + Isolation Forest + Autoencoder + JA3       â”‚
 â”‚  SHAP Explainability + Threat Intel + Active Probing     â”‚
 â”‚  Auto-Response + SQLite Graph DB + Streamlit Dashboard   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


def run_simulation(broker, engine):
    """
    Generate simulated traffic and push through the event pipeline.

    This exercises the full system: generate â†’ analyze â†’ alert â†’ block.
    """
    logger.info("â”â”â” PHASE 1: Generating simulated traffic â”â”â”")
    df = generate_dataset(num_normal_users=20, num_shadow_users=5)
    logger.info("Generated %d flow records from %d IPs",
                len(df), df["source_ip"].nunique())

    # Train ML models on the dataset
    logger.info("â”â”â” PHASE 2: Training ML models â”â”â”")
    train_stats = engine.initialize(df)
    logger.info("Training complete: %s", train_stats)

    # Push each flow through the event pipeline
    logger.info("â”â”â” PHASE 3: Processing flows through event pipeline â”â”â”")
    all_ips = df["source_ip"].unique().tolist()
    total_flows = 0

    for ip in all_ips:
        ip_traffic = df[df["source_ip"] == ip]
        for _, row in ip_traffic.iterrows():
            flow = row.to_dict()
            # Ensure timestamp is string (JSON-safe)
            if hasattr(flow.get("timestamp"), "isoformat"):
                flow["timestamp"] = flow["timestamp"].isoformat()
            broker.publish("traffic.flow", flow)
            total_flows += 1

        if total_flows % 200 == 0:
            logger.info("Processed %d / %d flows...", total_flows, len(df))

    logger.info("â”â”â” COMPLETE: %d flows processed â”â”â”", total_flows)
    return df


def print_summary(engine, probe, response, store):
    """Print a summary of the pipeline run."""
    print("\n" + "=" * 60)
    print("             PIPELINE SUMMARY")
    print("=" * 60)

    e = engine.stats
    print(f"\n   Analyzer Engine")
    print(f"     Flows processed:    {e['flows_processed']}")
    print(f"     Alerts generated:   {e['alerts_generated']}")
    print(f"     JA3 blocks:         {e['ja3_blocks']}")
    print(f"     ML trained:         {e['ml_trained']}")
    print(f"     AE trained:         {e['ae_trained']}")

    p = probe.stats
    print(f"\n   Active Probe")
    print(f"     Probes sent:        {p['probes_sent']}")
    print(f"     AI confirmed:       {p['ai_confirmed']}")
    print(f"     Cached probes:      {p['cached_probes']}")

    r = response.stats
    print(f"\n   Response Manager")
    print(f"     IPs blocked:        {r['blocked_count']}")
    print(f"     Audit entries:      {r['audit_entries']}")
    if r['blocked_ips']:
        for ip in r['blocked_ips']:
            print(f"       â”œâ”€â”€ {ip}")

    s = store.stats()
    print(f"\n   Graph Store (SQLite)")
    print(f"     Nodes:              {s['nodes']}")
    print(f"     Edges:              {s['edges']}")
    print(f"     Events:             {s['events']}")

    b = broker_ref.stats()
    print(f"\n   Event Broker")
    print(f"     Active topics:      {b['topics']}")
    for topic, count in b['history_sizes'].items():
        print(f"       â”œâ”€â”€ {topic}: {count} events")

    print("\n" + "=" * 60)
    print(f"    Pipeline finished at {datetime.now().strftime('%H:%M:%S')}")
    print(f"    Database: shadow_hunter.db")
    print(f"    Dashboard: streamlit run dashboard.py")
    print("=" * 60 + "\n")


# Global ref for summary printing
broker_ref = None


def main():
    global broker_ref

    parser = argparse.ArgumentParser(
        description="Shadow Hunter â€” Unified Architecture Runner"
    )
    parser.add_argument(
        "--sim", action="store_true",
        help="Run with simulated traffic (default if no mode specified)",
    )
    parser.add_argument(
        "--live", action="store_true",
        help="Run with live packet capture (requires Npcap/Scapy)",
    )
    parser.add_argument(
        "--reset", action="store_true",
        help="Reset database before running",
    )
    parser.add_argument(
        "--no-defense", action="store_true",
        help="Disable active probing and auto-response",
    )
    args = parser.parse_args()

    # Default to sim mode
    if not args.sim and not args.live:
        args.sim = True

    banner()

    # â”€â”€ Step 1: Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Initializing infrastructure...")
    broker = MemoryBroker(history_size=10000)
    broker_ref = broker

    store = SQLiteGraphStore(db_path="shadow_hunter.db", reset=args.reset)

    # â”€â”€ Step 2: Services â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Initializing services...")
    engine = AnalyzerEngine(
        broker=broker,
        store=store,
        active_defense=not args.no_defense,
    )

    probe = ActiveProbe(
        broker=broker,
        enabled=not args.no_defense,
    )

    response = ResponseManager(
        broker=broker,
        ttl_seconds=3600,
        enabled=not args.no_defense,
    )

    logger.info("All services initialized âœ“")

    # â”€â”€ Step 3: Ingestion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.sim:
        logger.info("Mode: SIMULATION")
        df = run_simulation(broker, engine)
    elif args.live:
        logger.info("Mode: LIVE CAPTURE")
        try:
            from scapy.all import sniff, IP, TCP
            logger.info("Scapy loaded â€” starting live capture...")
            print("ğŸ”´ Live capture not yet wired. Use --sim for now.")
            sys.exit(1)
        except ImportError:
            logger.error("Scapy not installed. Install with: pip install scapy")
            logger.error("Also need Npcap: https://npcap.com")
            sys.exit(1)

    # â”€â”€ Step 4: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print_summary(engine, probe, response, store)

    # Cleanup
    store.close()


if __name__ == "__main__":
    main()
